{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "34f902df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pca import *\n",
    "from main import getLineidxs\n",
    "from readply import expandIndices, getIndices, getNearestDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "d264d6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcSpecificModel(eigenValues, mean, eigenVectors):\n",
    "    U_k = eigenVectors[:, 0:30]\n",
    "    x_redBase = copyList(eigenValues)\n",
    "    x_red = copyList(eigenValues)\n",
    "    print('eigenvalues shape:' , np.array(eigenValues).shape)\n",
    "    basePCAmodel = (np.dot(U_k, x_redBase) + mean).real\n",
    "    adjustedModel = (np.dot(U_k, x_red) + mean).real\n",
    "    cutoff = 5\n",
    "    indices = rp.getIndices('breast_indices/leftBreastIdxLargeArea.txt') + rp.getIndices('breast_indices/rightBreastIdxLargeArea.txt')\n",
    "    outline = rp.getIndices('breast_indices/leftBreastIdxLargeAreaOutline.txt') + rp.getIndices('breast_indices/rightBreastIdxLargeAreaOutline.txt')\n",
    "    indices = list(set(indices))\n",
    "    dists = []\n",
    "    for i in indices:\n",
    "        dists.append(rp.getNearestDist(outline, adjustedModel, adjustedModel[i*3], adjustedModel[i*3+1], adjustedModel[i*3+2]))\n",
    "\n",
    "    indices_exp = rp.expandIndices(indices)\n",
    "    for i in range(len(indices_exp)):\n",
    "        dist = dists[int(i/3)]\n",
    "        if dist > cutoff:\n",
    "            basePCAmodel[indices_exp[i]] = adjustedModel[indices_exp[i]]\n",
    "        else:\n",
    "            basePCAmodel[indices_exp[i]] = (dist/cutoff) * adjustedModel[indices_exp[i]] + (1-(dist/cutoff)) * basePCAmodel[indices_exp[i]]\n",
    "    return basePCAmodel\n",
    "\n",
    "def copyList(l):\n",
    "    r = []\n",
    "    for i in range(len(l)):\n",
    "        r.append(l[i])\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "c16fb3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lines = pd.read_csv('lines.txt', sep=' ', header=None)\n",
    "df_params = pd.read_csv('params.txt', sep=' ', header=None)\n",
    "eigenVectors = read_matrix_from_hdf5('eigenVectors.h5')\n",
    "mean = read_matrix_from_hdf5('mean.h5')\n",
    "leftLineIdxs, rightLineIdxs = getLineidxs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "2b28d714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 144])\n"
     ]
    }
   ],
   "source": [
    "eigenValuesAugmentation_fit = [37.47394805, -49.07828283,  -0.6562043,  -24.62020633, -27.13628352, 6.57057748, -11.42703975, -18.79096601, -10.54424185, 3.44637848, -12.89579981, -7.87242574, -13.9092693, 2.27681137, -7.3060829, -7.73122333, -4.43121648, 5.80548907, 10.15422231, 14.24570948, -17.25150115, 0.97727416, -3.08962112, 5.8461985, 6.87403008, -3.17929581, 1.36895233, -0.74191196, -0.60536829, 5.94243889]\n",
    "for i in range(30):\n",
    "    df_lines[114+i] = [eigenValuesAugmentation_fit[i]] * len(df_lines)\n",
    "train_data = df_lines.sample(frac=0.8,random_state=200)\n",
    "test_data = df_lines.drop(train_data.index)\n",
    "x_train = train_data\n",
    "y_train = train_data.iloc[:, 0:114]\n",
    "x_test = test_data\n",
    "y_test = test_data.iloc[:, 0:114]\n",
    "\n",
    "x_train = torch.FloatTensor(x_train.values)\n",
    "x_test = torch.FloatTensor(x_test.values)\n",
    "y_train = torch.LongTensor(y_train.values)\n",
    "y_test = torch.LongTensor(y_test.values)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "fb380aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(144, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 32)\n",
    "        self.out = nn.Linear(32, 30)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        return self.out(x)\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, output, target):\n",
    "        num = output.shape[0]\n",
    "        sums = 0\n",
    "        for i in range(num):\n",
    "            nnOut = output.detach().numpy()[i,:]\n",
    "            tensTarget = target.detach().numpy()[i,:]\n",
    "            model = calcSpecificModel(nnOut, mean, eigenVectors)\n",
    "            idxs = expandIndices(rightLineIdxs)\n",
    "            sum = 0\n",
    "            for i in range(0,len(idxs),3):\n",
    "                sums += np.sqrt((model[idxs[i]] - tensTarget[i])**2 + (model[idxs[i+1]] - tensTarget[i+1])**2 + (model[idxs[i+2]] - tensTarget[i+2])**2)\n",
    "            #sums.append(sum)\n",
    "        \n",
    "        return torch.tensor(sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "bb0764e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (fc1): Linear(in_features=144, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc5): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (out): Linear(in_features=32, out_features=30, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "myModel = Model()\n",
    "print(myModel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "a13e5f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    myModel.eval()\n",
    "    y_pred = myModel.forward(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "2292a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "critereon = CustomLoss()\n",
    "optimizer = torch.optim.Adam(myModel.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "565044d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 30])\n",
      "torch.Size([20, 114])\n",
      "eigenvalues shape: (30,)\n",
      "eigenvalues shape: (30,)\n",
      "eigenvalues shape: (30,)\n",
      "eigenvalues shape: (30,)\n",
      "eigenvalues shape: (30,)\n",
      "eigenvalues shape: (30,)\n",
      "eigenvalues shape: (30,)\n",
      "eigenvalues shape: (30,)\n",
      "eigenvalues shape: (30,)\n",
      "eigenvalues shape: (30,)\n",
      "eigenvalues shape: (30,)\n",
      "eigenvalues shape: (30,)\n",
      "eigenvalues shape: (30,)\n",
      "eigenvalues shape: (30,)\n",
      "eigenvalues shape: (30,)\n",
      "eigenvalues shape: (30,)\n",
      "eigenvalues shape: (30,)\n",
      "eigenvalues shape: (30,)\n",
      "eigenvalues shape: (30,)\n",
      "eigenvalues shape: (30,)\n",
      "epoch:  0  loss: 1209.46830949\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[287], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mepoch: \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m:\u001b[39;00m\u001b[39m2\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m  loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m10.8f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 13\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     14\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    y_pred = myModel.forward(x_train)\n",
    "    print(y_pred.shape)\n",
    "    print(y_train.shape)\n",
    "    loss = critereon(y_pred, y_train)\n",
    "    losses.append(loss)\n",
    "    print(f'epoch: {i:2}  loss: {loss.item():10.8f}')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
